---
title: "Capstone"
author: "Patrick Dunnington"
date: "2023-11-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(tidytext)
library(SnowballC)
library(topicmodels)
library(ggplot2)
library(stm)
library(LDAvis)
```
```{r}
incidents = read.csv("/Users/patrickdunnington/Desktop/DS_Capstone/mongodump/incidents_clean.csv")
#detail_incidents = data.frame(incidents$description)
#company_incidents = data.frame(incidents$Alleged.deployer.of.AI.system)
incidents$date = ymd(incidents$date)
incidents <- incidents[,-1]
```

```{r}
#cleaning dataset to utilize in python
for (i in 1:564){
    incidents$reportnumber[i] <- str_count(incidents$reports[i], ",") + 1 
}

#don't need to write unless downloading new file
#write.csv(incidents, file = "/Users/patrickdunnington/Desktop/DS_Capstone/mongodump/incidents_clean.csv", row.names = FALSE)

```

```{r}
#do LDA and topic modeling on incident description
tidy_incident_text <- incidents %>% 
  select(date, description) %>%
  unnest_tokens("word",description)
head(tidy_incident_text)
```
```{r}
#remove stop words, numbers and whitespace (articles that don't contribute real meaning)

data("stop_words") 
tidy_incident_text <- tidy_incident_text %>%
  anti_join(stop_words)

tidy_incident_text <- tidy_incident_text[-grep("\\b\\d+\\b", tidy_incident_text$word),]

#tidy_incident_text$word <- #gsub("\\s+","",tidy_incident_text$word)

#tidy_incident_text <- tidy_incident_text %>%
#  mutate(word = wordStem(word, language = "en"))

tidy_incident_text %>%
  count(word) %>%
  arrange(desc(n))
```
```{r}
incident_DTM <- tidy_incident_text %>% 
  count(date, word) %>%
  cast_dtm(date, word, n)
incident_DTM
```
```{r}
Incident_topic_model <- LDA(incident_DTM, k=10, control = list(seed = 321))
```

```{r}
incident_topics <- tidy(Incident_topic_model, matrix = "beta")
incident_top_terms <- incident_topics %>%
  group_by(topic)%>%
    top_n(10,beta)%>%
      ungroup()%>%
        arrange(topic, -beta)
incident_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales="free") + 
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5, size=18)) + 
  labs(title = "Topic Model of AI Incident Reports", caption = "Top Terms by Topic (betas)")+
  ylab("")+
  xlab("")+
  coord_flip(
  )
```
```{r}
# Load the data from CSV
incidents_for_STM <- read.csv("/Users/patrickdunnington/Desktop/DS_Capstone/mongodump/incidents_clean.csv", stringsAsFactors = FALSE)
incidents_for_STM$date <- as.numeric(incidents_for_STM$date)
  
incident_processed <- textProcessor(incidents_for_STM$description, metadata = incidents)
out <- prepDocuments(incident_processed$documents, incident_processed$vocab, incident_processed$meta)
#create a structural topic model

```
```{r}
# prevalence =- date, max.em.its = 75, data = out$meta, init.type = "Spectral",
incident_STM <- stm(documents = out$documents, vocab = out$vocab, K=20, gamma.prior='L1',
                    prevalence =~ Alleged.developer.of.AI.system + s(date), data = out$meta, 
                    init.type = "Spectral", verbose = FALSE)
```

```{r}
#findingk <- searchK(out$documents, out$vocab, K=c(10,30), data = out$meta, verbose = FALSE)
#plot(findingk)
plot(incident_STM)
```
```{r}
#use to understand what topics describe
findThoughts(incident_STM, texts = incidents$description, n = 4, topics = 11)
```
```{r}
#predict topic relation to metadata
'''predict_topics <- estimateEffect(formula = 1:10 ~ Alleged.developer.of.AI.system + s(date), stmobj = incident_STM, metadata = out$meta, uncertainty = "Global")

plot(predict_topics, "date", method = "continuous", topics = 5, model = z,
     printlegend = FALSE, xaxt = "n", xlab = "Time (2010-2024)")
     monthseq <- seq(from = as.Date("2010-01-01"), to =
                       as.Date("2024-01-01"), by = "month")
     monthnames <- months(monthseq)
     axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)), labels
          = monthnames)'''

# Assuming stm_model is your STM object
# Extract topic prevalence over time
#topic_prevalence <- summary(incident_STM, type = "prevalence")
```

```{r}
interactive_topic <- toLDAvis(mod=incident_STM, docs=out$documents, R = 20)
```
```{r}
my_ldavis <- createJSON(phi = incident_STM$beta$logbeta, theta = incident_STM$theta, doc.length = out$documents)

#save_html(my_ldavis, file = "my_stm_ldavis_visualization.html")

```

```{r}
# Assuming your dataframe is named 'your_df'
library(dplyr)

# Select a random sample of 10 rows
sampled_df <- incidents %>% sample_n(10)

ggplot(sampled_df, aes(x = date, y = Alleged.deployer.of.AI.system)) +
  geom_point() 
  #facet_wrap(~island)

```